{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The_Monkey_Cage_v02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMes/TVyjYjzRKjOw+ptLjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apspatz/TheMonkeyCage/blob/master/The_Monkey_Cage_v02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LoNlYkvg42H",
        "colab_type": "text"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Given a sequence of characters from this data (e.g. \"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly. \n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTXF0I6wYcpH",
        "colab_type": "text"
      },
      "source": [
        "This code borrows strongly from TensorFlow's tutorial on text generation using an RNN. For more information please visit TensorFlow's website directly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBuiTDzvYl4M",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/text/text_generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FWnke1kZCYk",
        "colab_type": "text"
      },
      "source": [
        "Setup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4-fPTSNYja1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "272d08e9-46f8-4fe4-ba08-8676f2f50446"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3MivnuEZPGA",
        "colab_type": "text"
      },
      "source": [
        "Paths to the Homer text file (which contains both the Odyssey & the Illiad):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAbF0F6JY_eG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ae835dfe-4a55-421f-92ca-d337920c1006"
      },
      "source": [
        "path_to_file_homer = tf.keras.utils.get_file('HomerComplete_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/HomerComplete_edited.txt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/HomerComplete_edited.txt\n",
            "1458176/1456151 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j3ixn-2ZTuC",
        "colab_type": "text"
      },
      "source": [
        "Path to the Shakespeare text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtW2N9EpZJBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "039cf493-669e-4dd8-faf1-c58e318a2bb6"
      },
      "source": [
        "path_to_file_shakespeare = tf.keras.utils.get_file('shakespeare_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/shakespeare_edited.txt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/t8.shakespeare.txt\n",
            "5464064/5458199 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpytV-_apwS",
        "colab_type": "text"
      },
      "source": [
        "Path to the Bible text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iTWX2BaacIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "515154ee-2238-4efc-857d-cecee480311d"
      },
      "source": [
        "path_to_file_bible = tf.keras.utils.get_file('bible_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/bible_edited.txt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/bible.txt\n",
            "4366336/4365973 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWA52YL_cUa6",
        "colab_type": "text"
      },
      "source": [
        "Path to the Sherlock text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXqoKjffayGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "22bd2152-3767-4c2d-fd69-4096cde77381"
      },
      "source": [
        "path_to_file_sherlock = tf.keras.utils.get_file('sherlock_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/sherlock_edited.txt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/sherlock.txt\n",
            "614400/609377 [==============================] - 1s 2us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCzL2pJ8czqF",
        "colab_type": "text"
      },
      "source": [
        "Path to the Dante text files (containing The Divine Comedy, Paradise, The Divine Comedy, Purgatory, and The Vision of Hell):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEP2LUNScN4W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4bccfe1e-9be9-45d9-9949-c8be570cb4ec"
      },
      "source": [
        "path_to_file_dante = tf.keras.utils.get_file('DivineComedyComplete_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/DivineComedyComplete_edited.txt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/DivineComedyComplete.txt\n",
            "622592/622204 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJh1T9oZdQmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98a470d4-4b46-4dc8-d938-79b427f46a79"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file_homer, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of Homer text: {} characters'.format(len(text)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Homer text: 1445041 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvEhcg9ieOGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "dbdb23f0-51d8-4101-d57f-02bd10b817ff"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK I\n",
            "\n",
            "Sing, O goddess, the anger of Achilles son of Peleus, that brought\n",
            "countless ills upon the Achaeans. Many a brave soul did it send hurrying\n",
            "down to Hades, and many a hero did it yield a prey to dogs and vultures,\n",
            "for so were the counsels of J\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTffLoDyed-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "19e10664-dd3c-4157-f6cc-5ee50fad1c03"
      },
      "source": [
        "print(text[-550:])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "o earth, and some confusedly fly.\n",
            "With dreadful shouts Ulysses pour’d along,\n",
            "Swift as an eagle, as an eagle strong.\n",
            "But Jove’s red arm the burning thunder aims:\n",
            "Before Minerva shot the livid flames;\n",
            "Blazing they fell, and at her feet expired;\n",
            "Then stopped the goddess, trembled and retired.\n",
            "\n",
            "“Descended from the gods! Ulysses, cease;\n",
            "Offend not Jove: obey, and give the peace.”\n",
            "\n",
            "So Pallas spoke: the mandate from above\n",
            "The king obey’d. The virgin-seed of Jove,\n",
            "In Mentor’s form, confirm’d the full accord,\n",
            "And willing nations knew their lawful lord.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl_XiuaeelEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text = text[251:-555]\n",
        "# print(text[0:250])\n",
        "# print(text[-250:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E01Ayl2SfHmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59da25a4-8c7d-453e-8ef0-c7fe1f45a10d"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_S631igDGk",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdrEepl6ftrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57XCuuk2ibie",
        "colab_type": "text"
      },
      "source": [
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLna9xOignLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f79e22fb-a756-42c4-9f29-1346421577d7"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B\n",
            "O\n",
            "O\n",
            "K\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DjiF-3Uiiac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFcNDQkkmruk",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0k38HlPme9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5nW7Y1UmwLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a4f7854f-8e0e-433d-d11f-91443d961fb6"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'BOOK I\\n\\nSing, O goddess, the anger of Achilles son of Peleus, that brought\\ncountless ills upon the A'\n",
            "Target data: 'OOK I\\n\\nSing, O goddess, the anger of Achilles son of Peleus, that brought\\ncountless ills upon the Ac'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiaM_qdQnnWD",
        "colab_type": "text"
      },
      "source": [
        "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"B\" and trys to predict the index for \"O\" as the next character (in the case of using Homer text file). At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8nFTKXgmzmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "734d3580-59ad-4f7b-e5a4-0ead1ce10c62"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 15 ('B')\n",
            "  expected output: 28 ('O')\n",
            "Step    1\n",
            "  input: 28 ('O')\n",
            "  expected output: 28 ('O')\n",
            "Step    2\n",
            "  input: 28 ('O')\n",
            "  expected output: 24 ('K')\n",
            "Step    3\n",
            "  input: 24 ('K')\n",
            "  expected output: 1 (' ')\n",
            "Step    4\n",
            "  input: 1 (' ')\n",
            "  expected output: 22 ('I')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2rW3Q-rnxVL",
        "colab_type": "text"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SsblwwYnsFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee9e90b5-d042-4d39-b2a6-6c3e64443fe3"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cZy5m1r_Ti",
        "colab_type": "text"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_NhQzELsC_z",
        "colab_type": "text"
      },
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0l90lgnz8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42MQICeosIIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1BfRU8VsKvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYT_AtbosYuw",
        "colab_type": "text"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKNfDVf3sO2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7964a87c-7a9c-4fa4-afa4-607134751151"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 72) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co5uSTkAsXJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "b77a4d36-29e8-45ef-911c-fca790335144"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           18432     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 72)            73800     \n",
            "=================================================================\n",
            "Total params: 4,030,536\n",
            "Trainable params: 4,030,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S5l9HcEsyjg",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpgLNzsjs2Kc",
        "colab_type": "text"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4A2hxpYs4fz",
        "colab_type": "text"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMo0KiKusiRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c4e4af61-4939-430e-a243-5bc0bc9154a4"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 72)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.277217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84uW2FF4tX6J",
        "colab_type": "text"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWObca8tRvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNCukYitjl2",
        "colab_type": "text"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUnhK5vNta4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6iXY8bJtvFO",
        "colab_type": "text"
      },
      "source": [
        "### Execute the Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrWAmBxPt8UG",
        "colab_type": "text"
      },
      "source": [
        "In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9kJ8qkDtmA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwZy_UwluAP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0300c45-5f6e-4777-a292-031226bcb697"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 223 steps\n",
            "Epoch 1/40\n",
            "223/223 [==============================] - 15s 66ms/step - loss: 2.4805\n",
            "Epoch 2/40\n",
            "223/223 [==============================] - 14s 62ms/step - loss: 1.8315\n",
            "Epoch 3/40\n",
            "223/223 [==============================] - 14s 63ms/step - loss: 1.5722\n",
            "Epoch 4/40\n",
            "223/223 [==============================] - 14s 65ms/step - loss: 1.4418\n",
            "Epoch 5/40\n",
            "223/223 [==============================] - 15s 67ms/step - loss: 1.3643\n",
            "Epoch 6/40\n",
            "223/223 [==============================] - 15s 67ms/step - loss: 1.3081\n",
            "Epoch 7/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 1.2621\n",
            "Epoch 8/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 1.2214\n",
            "Epoch 9/40\n",
            "223/223 [==============================] - 16s 70ms/step - loss: 1.1840\n",
            "Epoch 10/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 1.1478\n",
            "Epoch 11/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 1.1131\n",
            "Epoch 12/40\n",
            "223/223 [==============================] - 15s 67ms/step - loss: 1.0799\n",
            "Epoch 13/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 1.0464\n",
            "Epoch 14/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 1.0127\n",
            "Epoch 15/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 0.9820\n",
            "Epoch 16/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 0.9505\n",
            "Epoch 17/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 0.9217\n",
            "Epoch 18/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 0.8955\n",
            "Epoch 19/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.8718\n",
            "Epoch 20/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.8497\n",
            "Epoch 21/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.8301\n",
            "Epoch 22/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.8150\n",
            "Epoch 23/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7994\n",
            "Epoch 24/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7873\n",
            "Epoch 25/40\n",
            "223/223 [==============================] - 15s 68ms/step - loss: 0.7758\n",
            "Epoch 26/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7668\n",
            "Epoch 27/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7585\n",
            "Epoch 28/40\n",
            "223/223 [==============================] - 16s 70ms/step - loss: 0.7535\n",
            "Epoch 29/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7471\n",
            "Epoch 30/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7428\n",
            "Epoch 31/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7379\n",
            "Epoch 32/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7349\n",
            "Epoch 33/40\n",
            "223/223 [==============================] - 16s 70ms/step - loss: 0.7327\n",
            "Epoch 34/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7329\n",
            "Epoch 35/40\n",
            "223/223 [==============================] - 16s 71ms/step - loss: 0.7320\n",
            "Epoch 36/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7328\n",
            "Epoch 37/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7313\n",
            "Epoch 38/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7327\n",
            "Epoch 39/40\n",
            "223/223 [==============================] - 16s 70ms/step - loss: 0.7324\n",
            "Epoch 40/40\n",
            "223/223 [==============================] - 15s 69ms/step - loss: 0.7351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9XSeKa2uN7-",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBnfctUuQuC",
        "colab_type": "text"
      },
      "source": [
        "### Restore the latest checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfEJl5zMuaQf",
        "colab_type": "text"
      },
      "source": [
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsoUoiREuCvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f4d0441-3c03-4df1-caf3-829d60ea52a0"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_40'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlVnvddduenk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07FyOAWdugdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "e4bda078-fd02-4cad-e340-f8b6bcb0d763"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            18432     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 72)             73800     \n",
            "=================================================================\n",
            "Total params: 4,030,536\n",
            "Trainable params: 4,030,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKaM0G6ujqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Yv1GWn16Al",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "bdb68c18-b308-4b5c-a818-627455c97900"
      },
      "source": [
        "print(generate_text(model, start_string=u\"The sea \"))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sea spear, I came, but some neward the retireoid\n",
            "the ways of way,\n",
            "And each be fill’d on every side from out about\n",
            "the wall. It was yet at escape from which he strong far others to display’d Hector; he prowe\n",
            "So year, a vase supplies,\n",
            "On wove, Saschet: the hero ran,\n",
            "White swains the vestur send to Agamemnon; but the\n",
            "young men away, and that you were struck\n",
            "with the obscures of Mt. Placus to the host of kind);\n",
            "Whose boughs secused full of gone, the prince with state arphichos\n",
            "strongly rush in wrantly on his body. Hector\n",
            "was nearolong up all was both to chase at accountely, by the stern of\n",
            "the son of Atreus coming through the gooder gening by the roads, at once,\n",
            "and your sire crossed the chariot. Menelaus then tole\n",
            "him with embattled, \"which of the Lycianae,\n",
            "warrior- to easly into the finger of good in fumb, and I will gain noble kinddy hold his anger.\n",
            "It went to Troy as I can. The snatcher-banquety of battle with wind-carefully on the plain\n",
            "for battle? we will yet fell sprawling upon them, an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIOzNvRA2G5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}