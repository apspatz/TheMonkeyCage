{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The_Monkey_Cage_v03.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNAJVgf2y/2KFcPcd36fpF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apspatz/TheMonkeyCage/blob/master/The_Monkey_Cage_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LoNlYkvg42H",
        "colab_type": "text"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.\n",
        "\n",
        "Given a sequence of characters from this data (e.g. \"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly. \n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTXF0I6wYcpH",
        "colab_type": "text"
      },
      "source": [
        "This code borrows strongly from TensorFlow's tutorial on text generation using an RNN. For more information please visit TensorFlow's website directly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBuiTDzvYl4M",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/text/text_generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FWnke1kZCYk",
        "colab_type": "text"
      },
      "source": [
        "Setup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4-fPTSNYja1",
        "colab_type": "code",
        "outputId": "1de66769-c23c-46c0-8ccf-223e5153c277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3MivnuEZPGA",
        "colab_type": "text"
      },
      "source": [
        "Paths to the Homer text file (which contains both the Odyssey & the Illiad):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAbF0F6JY_eG",
        "colab_type": "code",
        "outputId": "5723398d-74f9-4ceb-e087-2360ddf5c935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_file_homer = tf.keras.utils.get_file('HomerComplete_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/HomerComplete_edited.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/HomerComplete_edited.txt\n",
            "1458176/1456151 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owV8g4kmTAVe",
        "colab_type": "text"
      },
      "source": [
        "Path to the Yoda text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6PT2taxS8a-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "680855f8-2e9e-49e9-8489-b0971d5595d3"
      },
      "source": [
        "path_to_file_yoda = tf.keras.utils.get_file('yoda_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/yoda_edited.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/yoda_edited.txt\n",
            "\r8192/6498 [=====================================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j3ixn-2ZTuC",
        "colab_type": "text"
      },
      "source": [
        "Path to the Shakespeare text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtW2N9EpZJBc",
        "colab_type": "code",
        "outputId": "80a2e970-81d8-4b69-ad1a-2f819cba5898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_file_shakespeare = tf.keras.utils.get_file('shakespeare_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/shakespeare_edited.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/shakespeare_edited.txt\n",
            "5464064/5458199 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpytV-_apwS",
        "colab_type": "text"
      },
      "source": [
        "Path to the Bible text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iTWX2BaacIW",
        "colab_type": "code",
        "outputId": "9c636a87-39db-4980-f338-2f27fe17c8d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_file_bible = tf.keras.utils.get_file('bible_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/bible_edited.txt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/bible_edited.txt\n",
            "4341760/4337219 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWA52YL_cUa6",
        "colab_type": "text"
      },
      "source": [
        "Path to the Sherlock text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXqoKjffayGS",
        "colab_type": "code",
        "outputId": "119ea86a-4958-4609-e781-aad1510fc580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_file_sherlock = tf.keras.utils.get_file('sherlock_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/sherlock_edited.txt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/sherlock_edited.txt\n",
            "581632/577376 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCzL2pJ8czqF",
        "colab_type": "text"
      },
      "source": [
        "Path to the Dante text files (containing The Divine Comedy, Paradise, The Divine Comedy, Purgatory, and The Vision of Hell):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEP2LUNScN4W",
        "colab_type": "code",
        "outputId": "1ee717a3-2838-4f0a-e67c-0185c17d4b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_file_dante = tf.keras.utils.get_file('DivineComedyComplete_edited.txt', 'https://mandrewsbucket.s3.amazonaws.com/DivineComedyComplete_edited.txt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://mandrewsbucket.s3.amazonaws.com/DivineComedyComplete_edited.txt\n",
            "622592/622204 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJh1T9oZdQmk",
        "colab_type": "code",
        "outputId": "a75753d5-d325-47a3-bb82-63a82c5835b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "# text = open(path_to_file_homer, 'rb').read().decode(encoding='utf-8')\n",
        "text = open(path_to_file_yoda, 'rb').read().decode(encoding='utf-8')\n",
        "# text = open(path_to_file_bible, 'rb').read().decode(encoding='utf-8')\n",
        "# text = open(path_to_file_dante, 'rb').read().decode(encoding='utf-8')\n",
        "# text = open(path_to_file_sherlock, 'rb').read().decode(encoding='utf-8')\n",
        "# text = open(path_to_file_shakespeare, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 6498 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvEhcg9ieOGd",
        "colab_type": "code",
        "outputId": "5a8a6da9-59d5-4304-96a7-67eb32bad8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The very Republic is threatened, if involved the Sith are.\r\n",
            "Hard to see, the dark side is. Discover who this assassin is, we must.\r\n",
            "With this Naboo queen you must stay, Qui-Gon. Protect her.\r\n",
            "May the Force be with you.\r\n",
            "(Contd) Master Qui-Gon more to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTffLoDyed-P",
        "colab_type": "code",
        "outputId": "9ed41133-e79c-4aa4-8907-85df12b05efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "print(text[-550:])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ven your physical self, perhaps.\r\n",
            "Hmm. That face you make. Look I so old to young eyes?\r\n",
            "Soon will I rest. Yes, forever sleep. Earned it, I have.\r\n",
            "Strong am I with the Force... but not that strong! Twilight is upon me \r\n",
            "No more training do you require. Already know you that which you need.\r\n",
            "Mmm... rest I need. Yes... rest.\r\n",
            "Your father he is.\r\n",
            "Told you, did he?\r\n",
            "Unexpected this is, and unfortunate...\r\n",
            "Remember, a Jedis strength flows from the Force.  But beware.  Anger, \r\n",
            "Luke...Luke...Do not...Do not underestimate the powers of the Emperor, \r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl_XiuaeelEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text = text[251:-555]\n",
        "# print(text[0:250])\n",
        "# print(text[-250:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E01Ayl2SfHmz",
        "colab_type": "code",
        "outputId": "c81bc2df-2c17-4d84-95fa-a5b685ffe412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_S631igDGk",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdrEepl6ftrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57XCuuk2ibie",
        "colab_type": "text"
      },
      "source": [
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLna9xOignLq",
        "colab_type": "code",
        "outputId": "01621928-1fcc-4eb3-cd3d-eb1e285f7d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "v\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DjiF-3Uiiac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "344bf11c-634f-4189-dd62-1da2d9516b7c"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'The very Republic is threatened, if involved the Sith are.\\r\\nHard to see, the dark side is. Discover w'\n",
            "'ho this assassin is, we must.\\r\\nWith this Naboo queen you must stay, Qui-Gon. Protect her.\\r\\nMay the Fo'\n",
            "'rce be with you.\\r\\n(Contd) Master Qui-Gon more to say have you?\\r\\nA vergence, you say?\\r\\nBut you do! Rre'\n",
            "'vealed your opinion is.\\r\\nTrained as a Jedi, you request for him?\\r\\nTested he will be.\\r\\nGood, good, you'\n",
            "'ng one. How feel you?\\r\\nAfraid are you?\\r\\nSee through you, we can.\\r\\nAfraid to lose her..I think.\\r\\nEveyt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFcNDQkkmruk",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0k38HlPme9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5nW7Y1UmwLr",
        "colab_type": "code",
        "outputId": "d526fab6-12f1-44e3-9bed-aa5d07891930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'The very Republic is threatened, if involved the Sith are.\\r\\nHard to see, the dark side is. Discover '\n",
            "Target data: 'he very Republic is threatened, if involved the Sith are.\\r\\nHard to see, the dark side is. Discover w'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiaM_qdQnnWD",
        "colab_type": "text"
      },
      "source": [
        "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"B\" and trys to predict the index for \"O\" as the next character (in the case of using Homer text file). At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8nFTKXgmzmG",
        "colab_type": "code",
        "outputId": "4bb5e4d7-8f28-4cde-e844-76f386f7bdc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 30 ('T')\n",
            "  expected output: 42 ('h')\n",
            "Step    1\n",
            "  input: 42 ('h')\n",
            "  expected output: 39 ('e')\n",
            "Step    2\n",
            "  input: 39 ('e')\n",
            "  expected output: 2 (' ')\n",
            "Step    3\n",
            "  input: 2 (' ')\n",
            "  expected output: 56 ('v')\n",
            "Step    4\n",
            "  input: 56 ('v')\n",
            "  expected output: 39 ('e')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2rW3Q-rnxVL",
        "colab_type": "text"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SsblwwYnsFH",
        "colab_type": "code",
        "outputId": "e0596989-f3fa-4647-9dc6-ea22cfcde865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cZy5m1r_Ti",
        "colab_type": "text"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_NhQzELsC_z",
        "colab_type": "text"
      },
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0l90lgnz8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42MQICeosIIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1BfRU8VsKvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYT_AtbosYuw",
        "colab_type": "text"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKNfDVf3sO2j",
        "colab_type": "code",
        "outputId": "b6837a2e-c371-4583-bc87-06cedc24b708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 60) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co5uSTkAsXJj",
        "colab_type": "code",
        "outputId": "0bb1e199-fa6e-41a7-f264-9b7293249340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           15360     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 60)            61500     \n",
            "=================================================================\n",
            "Total params: 4,015,164\n",
            "Trainable params: 4,015,164\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S5l9HcEsyjg",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpgLNzsjs2Kc",
        "colab_type": "text"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4A2hxpYs4fz",
        "colab_type": "text"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMo0KiKusiRO",
        "colab_type": "code",
        "outputId": "8e07ca14-567f-4e74-ae0d-3a8be2e7f862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 60)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.095041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84uW2FF4tX6J",
        "colab_type": "text"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWObca8tRvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNCukYitjl2",
        "colab_type": "text"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUnhK5vNta4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6iXY8bJtvFO",
        "colab_type": "text"
      },
      "source": [
        "### Execute the Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrWAmBxPt8UG",
        "colab_type": "text"
      },
      "source": [
        "In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9kJ8qkDtmA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwZy_UwluAP1",
        "colab_type": "code",
        "outputId": "27793d0c-9605-4ea4-b0c1-6833c06f7907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1 steps\n",
            "Epoch 1/200\n",
            "\r1/1 [==============================] - 0s 193ms/step - loss: 0.0279\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0278\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0253\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0266\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0274\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.0247\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0256\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0282\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0279\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.0252\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0272\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0279\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0281\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.0267\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0287\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0252\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0241\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0285\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.0281\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0257\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0270\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0235\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.0241\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.0261\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0246\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.0258\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0252\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0256\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.0248\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0271\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0232\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0232\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0242\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0219\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.0265\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0229\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0238\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 1s 920ms/step - loss: 0.0246\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.0247\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0251\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0239\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 1s 801ms/step - loss: 0.0243\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0245\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0279\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.0215\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0226\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0227\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0210\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0253\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.0259\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0241\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0209\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 1s 907ms/step - loss: 0.0235\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0251\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0238\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0230\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0223\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.0213\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0220\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0231\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0260\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.0237\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0217\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0234\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0222\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0241\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0240\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0244\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0209\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0214\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0240\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0207\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.0211\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0197\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0227\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 0.0229\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.0215\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0235\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.0225\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0196\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0216\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0184\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 1s 917ms/step - loss: 0.0227\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0212\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.0199\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0209\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0187\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0177\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0207\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0206\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0230\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0237\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 1s 917ms/step - loss: 0.0223\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0267\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.0211\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0201\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0219\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0197\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0218\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.0207\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0226\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 1s 921ms/step - loss: 0.0213\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0230\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.0234\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0220\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0205\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 1s 637ms/step - loss: 0.0198\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0200\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0195\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0212\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0212\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.0223\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0224\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 1s 716ms/step - loss: 0.0221\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0196\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0202\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.0222\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0198\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0189\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 1s 918ms/step - loss: 0.0209\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0206\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.0223\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0195\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0209\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.0194\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0211\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.0218\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0214\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 1s 796ms/step - loss: 0.0187\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0206\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.0200\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0187\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0206\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 1s 805ms/step - loss: 0.0196\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0197\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.0235\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0222\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0211\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 1s 635ms/step - loss: 0.0217\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0185\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0189\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 1s 762ms/step - loss: 0.0234\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0198\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.0189\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0208\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0201\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 1s 631ms/step - loss: 0.0198\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0196\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.0213\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0209\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0207\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0181\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0178\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.0211\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0204\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 0.0217\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0173\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.0184\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0196\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0210\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0199\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.0219\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0210\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.0191\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0192\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0179\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 1s 769ms/step - loss: 0.0183\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0219\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.0197\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0182\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0167\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0191\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0177\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.0192\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.0188\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.0187\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.0192\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0187\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0184\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0166\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.0191\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0206\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0203\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0196\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.0173\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0172\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0195\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.0168\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0170\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0197\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0186\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9XSeKa2uN7-",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBnfctUuQuC",
        "colab_type": "text"
      },
      "source": [
        "### Restore the latest checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfEJl5zMuaQf",
        "colab_type": "text"
      },
      "source": [
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsoUoiREuCvT",
        "colab_type": "code",
        "outputId": "7d40e60f-e198-408b-b37f-084c93df3cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_200'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlVnvddduenk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07FyOAWdugdZ",
        "colab_type": "code",
        "outputId": "8c893901-7566-49cf-b8b0-9024c3ac9164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            15360     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 60)             61500     \n",
            "=================================================================\n",
            "Total params: 4,015,164\n",
            "Trainable params: 4,015,164\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKaM0G6ujqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Yv1GWn16Al",
        "colab_type": "code",
        "outputId": "756d22e4-fd27-46ce-bf4c-6726aca7b605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"The force\"))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The forced side.\r\n",
            " Death is a natural part of life. Rejoice for those around you who transform into the Force. a Jedis strength flows from the Force.  But beware.  Anger, \r\n",
            "Luke...Luke...Do not...Do not underestight our way.\r\n",
            " If a special session of Congress there is, easier for us to enter the Jedi Temple it scyou, the ceald fraining I have for you.\r\n",
            " An old friend has learned the path to immortality.\r\n",
            " One who has returned forever sleep. Earned it, I have.\r\n",
            "Strong am I with the Force... but not that strong! Twilight is upout, stysrave te be dear ofertroined, I have to say about it, Lord Sidious.\r\n",
            " (continuing) At an end your rule is and not short e Force, he will training I have to you?\r\n",
            " Careful you must be when sensing the future, Anakin. The fear of loss is a path to the dark side... fear leads to anger... anger leads to hate.. hate leads to hat ou be with toremmm... rest.\r\n",
            "Your father he is.\r\n",
            "Told you, did he?\r\n",
            "Unexpected this is, and unfortunate...\r\n",
            "Remember, is. Much anger there is in \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIOzNvRA2G5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ws4wxB4zIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5bcb0d6-6db9-4243-b06f-1fc73ee96d1c"
      },
      "source": [
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V21oYj6M41SH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}